extern "C" {
    #include "mm.h"
    #include "log.h"
    #include "defs.h"
    #include "string.h"
}

extern "C" {
    void setup_vm();
    void setup_vm_final();
}

// generated by vmlinux.lds.S
extern uint64 _stext;
extern uint64 _etext;
extern uint64 _sdata;
extern uint64 _edata;
extern uint64 _sbss;
extern uint64 _ebss;
extern uint64 _srodata;
extern uint64 _erodata;
extern uint64 _ekernel;

using uint64 = unsigned long;

// early_pgtbl: used for 1GB mapping of setup_vm
unsigned long early_pgtbl[512] __attribute__((__aligned__(0x1000)));

unsigned long swapper_pg_dir[512] __attribute__((__aligned__(0x1000)));

// protection bits of Page Table Entries: | RSW |D|A|G|U|X|W|R|V|
// set bit V | R | W | X to 1
constexpr uint64 PTE_VRWX = 0b00'0000'1111;
// set bit V to 1
constexpr uint64 PTE_V = 0b00'0000'0001;
// set bit V | R | X to 1
constexpr uint64 PTE_VRX = 0b00'0000'1011;
// set bit V | R to 1
constexpr uint64 PTE_VR = 0b00'0000'0011;
// set bit V | R | W to 1
constexpr uint64 PTE_VRW = 0b00'0000'0111;

constexpr uint64 PTE_FLAGS_LEN = 10;

constexpr uint64 PHY_START_CPP = static_cast<uint64>(PHY_START);
constexpr uint64 VM_START_CPP = static_cast<uint64>(VM_START);
constexpr uint64 PA2VA_OFFSET_CPP = static_cast<uint64>(PA2VA_OFFSET);

// length of page offset for both virtual and physical address
// PTE: |  44 PPN  |  10 flags |
constexpr uint64 PAGE_OFFSET_lEN = 12;
constexpr uint64 PAGE_INDEX_LEN = 9;

consteval auto generate_mask(const uint64 len) -> uint64 {
    return (1 << len) - 1;
}

inline auto va_to_pa(const uint64 va) -> uint64 {
    return va - PA2VA_OFFSET_CPP;
}

inline auto align_diff_to_pgsize(const uint64 start, const uint64 end) -> uint64 {
    return ((end - start) / PGSIZE + !!(end % PGSIZE)) * PGSIZE;
}

auto panic(char *content) -> void {
    log_failed(content);
    __asm__ volatile("ebreak");
};

constexpr auto get_pgtbl_index(const uint64 va, const uint64 level) -> uint64 {
    switch (level) {
        case 1:
            return va
                    >> PAGE_OFFSET_lEN
                    >> PAGE_INDEX_LEN
                    >> PAGE_INDEX_LEN
                    &  generate_mask(PAGE_INDEX_LEN);
        case 2:
            return va
                    >> PAGE_OFFSET_lEN
                    >> PAGE_INDEX_LEN
                    &  generate_mask(PAGE_INDEX_LEN);
        case 3:
            return va
                    >> PAGE_OFFSET_lEN
                    &  generate_mask(PAGE_INDEX_LEN);
        default:
            panic(const_cast<char*>("Kernel panic: invalid page table level"));
            return 0;  // never reach here
    }
}

inline constexpr auto get_ppn_pgtbl_addr(const uint64 pte) -> uint64 {
    return pte >> PTE_FLAGS_LEN << PAGE_OFFSET_lEN;
}

inline constexpr auto set_pte(const uint64 page_addr, const uint64 flags) -> uint64 {
    return page_addr
            >> PAGE_OFFSET_lEN                 // get 44-bit PPN of PTE
            << PTE_FLAGS_LEN          // move left 10 bits for flags
            |  flags;                          // set V | R | W | X to 1
}

// perform a 1GB mapping, only use one level page table
auto setup_vm() -> void {
    memset(early_pgtbl, 0x0, PGSIZE);

    constexpr uint64 one_level_pte_content = set_pte(PHY_START_CPP, PTE_VRWX);

    // we split 64-bit va into: | high bit | 9 bit | 30 bit |
    // the middle 9 bits are used as index of early_pgtbl
    early_pgtbl[get_pgtbl_index(PHY_START_CPP, 1)] = one_level_pte_content;
    early_pgtbl[get_pgtbl_index(VM_START_CPP, 1)] = one_level_pte_content;

    log_ok(const_cast<char*>("First-time virtual memory mapping finished"));
}

// create multi-level page table mapping
// pgtbl: the address of base page table
// va, pa: the virtual and physical address to be mapped
// sz: the size of the mapping
// flags: the protection bits of the mapping
auto create_mapping(uint64* const pgtbl, const uint64 va, const uint64 pa, const uint64 sz, const uint64 flags) -> void {
    for (uint64 i = 0; i < sz; i += PGSIZE) {
        const auto current_va = va + i;
        const auto current_pa = pa + i;
        const auto level_1_index = get_pgtbl_index(current_va, 1);
        const auto level_2_index = get_pgtbl_index(current_va, 2);
        const auto level_3_index = get_pgtbl_index(current_va, 3);
    
        if ((pgtbl[level_1_index] & generate_mask(PTE_FLAGS_LEN)) != PTE_V) {
            uint64 new_pgtbl = kalloc();
            memset(reinterpret_cast<void*>(new_pgtbl), 0x0, PGSIZE);
            pgtbl[level_1_index] = set_pte(va_to_pa(new_pgtbl) , PTE_V);
        }

        auto* const level_2_pgtbl_ptr = reinterpret_cast<uint64 *>(get_ppn_pgtbl_addr(pgtbl[level_1_index]));

        if ((level_2_pgtbl_ptr[level_2_index] & generate_mask(PTE_FLAGS_LEN)) != PTE_V) {
            uint64 new_pgtbl = kalloc();
            memset(reinterpret_cast<void*>(new_pgtbl), 0x0, PGSIZE);
            level_2_pgtbl_ptr[level_2_index] = set_pte(va_to_pa(new_pgtbl), PTE_V);
        }

        auto* const level_3_pgtbl_ptr = reinterpret_cast<uint64 *>(get_ppn_pgtbl_addr(level_2_pgtbl_ptr[level_2_index]));

        level_3_pgtbl_ptr[level_3_index] = set_pte(current_pa, flags);
    }
}

// perform a 128MB mapping, using three level page table
// and considering kernel segments 
auto setup_vm_final() -> void {
    auto stext_addr = reinterpret_cast<uint64>(&_stext);
    auto etext_addr = reinterpret_cast<uint64>(&_etext);
    auto sdata_addr = reinterpret_cast<uint64>(&_sdata);
    auto edata_addr = reinterpret_cast<uint64>(&_edata);
    auto sbss_addr = reinterpret_cast<uint64>(&_sbss);
    auto ebss_addr = reinterpret_cast<uint64>(&_ebss);
    auto srodata_addr = reinterpret_cast<uint64>(&_srodata);
    auto erodata_addr = reinterpret_cast<uint64>(&_erodata);
    auto ekernel_addr = reinterpret_cast<uint64>(&_ekernel);

    memset(swapper_pg_dir, 0x0, PGSIZE);

    // No OpenSBI mapping required

    // mapping kernel text X|-|R|V
    create_mapping(swapper_pg_dir,
            stext_addr,
            va_to_pa(stext_addr),
            align_diff_to_pgsize(stext_addr, etext_addr),
            PTE_VRX
    );
    log_ok(const_cast<char*>(".text   virtual memory mapped, permission set to R-X"));

    // mapping kernel rodata -|-|R|V
    create_mapping(swapper_pg_dir,
            srodata_addr,
            va_to_pa(srodata_addr),
            align_diff_to_pgsize(srodata_addr, erodata_addr),
            PTE_VR
    );
    log_ok(const_cast<char*>(".rodata virtual memory mapped, permission set to R--"));

    // mapping other memory -|W|R|V
    create_mapping(swapper_pg_dir,
            sdata_addr,
            va_to_pa(sdata_addr),
            align_diff_to_pgsize(sdata_addr, edata_addr),
            PTE_VRW
    );
    log_ok(const_cast<char*>(".data   virtual memory mapped, permission set to RW-"));
    create_mapping(swapper_pg_dir,
            sbss_addr,
            va_to_pa(sbss_addr),
            align_diff_to_pgsize(sbss_addr, ebss_addr),
            PTE_VRW
    );
    log_ok(const_cast<char*>(".bss    virtual memory mapped, permission set to RW-"));

    const uint64 ekernel_rnd_up_addr = (ekernel_addr / PGSIZE + !!(ekernel_addr % PGSIZE)) * PGSIZE ;
    create_mapping(swapper_pg_dir,
            ekernel_rnd_up_addr,
            va_to_pa(ekernel_rnd_up_addr),
            align_diff_to_pgsize(ekernel_rnd_up_addr, VM_START + PHY_SIZE),
            PTE_VRW
    );
    log_ok(const_cast<char*>("Other   virtual memory mapped, permission set to RW-"));

    // set satp with swapper_pg_dir
    const uint64 satp_content = va_to_pa(reinterpret_cast<uint64>(swapper_pg_dir))
            >> PAGE_OFFSET_lEN
            |  RISCV_SV39_MODE_MASK; 
    __asm__ volatile (
        "mv t0, %[satp_content]\n"
        "csrw satp, t0"
        : 
        : [satp_content] "r" (satp_content)
        : "memory"
    );

    // flush TLB
    __asm__ volatile("sfence.vma zero, zero");

    log_ok(const_cast<char*>("TLB flushed, second-time virtual memory initialization succeeded"));

    return;
}
