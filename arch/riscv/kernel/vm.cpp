extern "C" {
    #include "mm.h"
    #include "defs.h"
    #include "string.h"
    #include "printk.h"
}

extern "C" {
    void setup_vm();
    void setup_vm_final();
}

// generated by vmlinux.lds.S
extern uint64 _stext;
extern uint64 _etext;
extern uint64 _sdata;
extern uint64 _edata;
extern uint64 _sbss;
extern uint64 _ebss;
extern uint64 _srodata;
extern uint64 _erodata;

using uint64 = unsigned long;

// early_pgtbl: used for 1GB mapping of setup_vm
unsigned long early_pgtbl[512] __attribute__((__aligned__(0x1000)));

unsigned long swapper_pg_dir[512] __attribute__((__aligned__(0x1000)));

// protection bits of Page Table Entries: | RSW |D|A|G|U|X|W|R|V|
// set bit V | R | W | X to 1
constexpr uint64 PTE_VRWX = 0b00'0000'1111;
// set bit V to 1
constexpr uint64 PTE_V = 0b00'0000'0001;
// set bit V | R | X to 1
constexpr uint64 PTE_VRX = 0b00'0000'1011;
// set bit V | R to 1
constexpr uint64 PTE_VR = 0b00'0000'0011;
// set bit V | R | W to 1
constexpr uint64 PTE_VRW = 0b00'0000'0111;

constexpr uint64 PTE_FLAGS_LEN = 10;

constexpr uint64 PHY_START_CPP = static_cast<uint64>(PHY_START);
constexpr uint64 VM_START_CPP = static_cast<uint64>(VM_START);
constexpr uint64 PA2VA_OFFSET_CPP = static_cast<uint64>(PA2VA_OFFSET);

// length of page offset for both virtual and physical address
// PTE: |  44 PPN  |  10 flags |
constexpr uint64 PAGE_OFFSET_lEN = 12;
constexpr uint64 PAGE_INDEX_LEN = 9;

consteval auto generate_mask(const uint64 len) -> uint64 {
    return (1 << len) - 1;
}

inline auto va_to_pa(const uint64 va) -> uint64 {
    return va - PA2VA_OFFSET_CPP;
}

inline auto align_diff_to_pgsize(const uint64 start, const uint64 end) -> uint64 {
    return ((end - start) / PGSIZE + !!(end % PGSIZE)) * PGSIZE;
}

auto panic() -> void {
    printk(RED "kernel panic!" NC);
    __asm__ volatile("ebreak");
};

constexpr auto get_pgtbl_index(const uint64 va, const uint64 level) -> uint64 {
    switch (level) {
        case 1:
            return va
                    >> PAGE_OFFSET_lEN
                    >> PAGE_INDEX_LEN
                    >> PAGE_INDEX_LEN
                    &  generate_mask(PAGE_INDEX_LEN);
        case 2:
            return va
                    >> PAGE_OFFSET_lEN
                    >> PAGE_INDEX_LEN
                    &  generate_mask(PAGE_INDEX_LEN);
        case 3:
            return va
                    >> PAGE_OFFSET_lEN
                    &  generate_mask(PAGE_INDEX_LEN);
        default:
            panic();
            return 0;  // never reach here
    }
}

inline constexpr auto get_ppn_pgtbl_addr(const uint64 pte) -> uint64 {
    return pte >> PTE_FLAGS_LEN << PAGE_OFFSET_lEN;
}

inline constexpr auto set_pte(const uint64 page_addr, const uint64 flags) -> uint64 {
    return page_addr
            >> PAGE_OFFSET_lEN                 // get 44-bit PPN of PTE
            << PTE_FLAGS_LEN          // move left 10 bits for flags
            |  flags;                          // set V | R | W | X to 1
}

// perform a 1GB mapping, only use one level page table
auto setup_vm() -> void {
    for (auto i = 0; i < 512; i++) {
        early_pgtbl[i] = 0;
    }

    constexpr uint64 one_level_pte_content = set_pte(PHY_START_CPP, PTE_VRWX);

    // we split 64-bit va into: | high bit | 9 bit | 30 bit |
    // the middle 9 bits are used as index of early_pgtbl
    early_pgtbl[get_pgtbl_index(PHY_START_CPP, 1)] = one_level_pte_content;
    early_pgtbl[get_pgtbl_index(VM_START_CPP, 1)] = one_level_pte_content;
}

// create multi-level page table mapping
// pgtbl: the address of base page table
// va, pa: the virtual and physical address to be mapped
// sz: the size of the mapping
// flags: the protection bits of the mapping
auto create_mapping(uint64* const pgtbl, const uint64 va, const uint64 pa, const uint64 sz, const uint64 flags) -> void {
    auto level_1_index = get_pgtbl_index(va, 1);
    auto level_2_index = get_pgtbl_index(va, 2);
    auto level_3_index = get_pgtbl_index(va, 3);
    
    if ((pgtbl[level_1_index] & generate_mask(PTE_FLAGS_LEN)) != PTE_V) {
        pgtbl[level_1_index] = set_pte(kalloc(), PTE_V);
    }

    auto* const level_2_pgtbl_ptr = reinterpret_cast<uint64 *>(get_ppn_pgtbl_addr(pgtbl[level_1_index]));

    if ((level_2_pgtbl_ptr[level_2_index] & generate_mask(PTE_FLAGS_LEN)) != PTE_V) {
        level_2_pgtbl_ptr[level_2_index] = set_pte(kalloc(), PTE_V);
    }

    auto* const level_3_pgtbl_ptr = reinterpret_cast<uint64 *>(get_ppn_pgtbl_addr(level_2_pgtbl_ptr[level_2_index]));
    
    if ((level_3_pgtbl_ptr[level_3_index] & generate_mask(PTE_FLAGS_LEN)) != PTE_V) {
        level_3_pgtbl_ptr[level_3_index] = set_pte(pa, flags);
    }

    for (uint64 i = 0; i < sz / PGSIZE; i++) {
        level_3_pgtbl_ptr[level_3_index + i] = set_pte(pa + i * PGSIZE, flags);
    }
}

auto setup_vm_final() -> void {
    memset(swapper_pg_dir, 0x0, PGSIZE);

    // No OpenSBI mapping required

    // mapping kernel text X|-|R|V
    create_mapping(swapper_pg_dir, _stext, va_to_pa(_stext), align_diff_to_pgsize(_stext, _etext), PTE_VRX);

    // mapping kernel rodata -|-|R|V
    create_mapping(swapper_pg_dir, _srodata, va_to_pa(_srodata), align_diff_to_pgsize(_srodata, _erodata), PTE_VR);

    // mapping other memory -|W|R|V
    create_mapping(swapper_pg_dir, _sdata, va_to_pa(_sdata), align_diff_to_pgsize(_sdata, _edata), PTE_VRW);
    create_mapping(swapper_pg_dir, _sbss, va_to_pa(_sbss), align_diff_to_pgsize(_sbss, _ebss), PTE_VR);

    // set satp with swapper_pg_dir
    const uint64 satp_content = reinterpret_cast<uint64>(swapper_pg_dir)
            >> PAGE_OFFSET_lEN 
            |  RISCV_SV39_MODE_MASK; 
    __asm__ volatile (
        "mv t0, %[satp_content]\n"
        "csrw satp, t0"
        : 
        : [satp_content] "r" (satp_content)
        : "memory"
    );

    // flush TLB
    __asm__ volatile("sfence.vma zero, zero");
    return;
}
